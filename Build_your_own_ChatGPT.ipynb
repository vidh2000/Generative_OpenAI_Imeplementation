{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RT3IysHWffn"
      },
      "source": [
        "# Building AI Assistants Part I: Build Your Own ChatGPT\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/life-efficient/A23/blob/main/1.%20Building%20AI%20Assistants%20Part%20I%3A%20Your%20Own%20ChatGPT/Solutions.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\n",
        "[Find the solutions here](https://colab.research.google.com/github/life-efficient/A23/blob/main/1.%20Building%20AI%20Assistants%20Part%20I%3A%20Your%20Own%20ChatGPT/Solutions.ipynb)\n",
        "\n",
        "[Find the slides here](https://tome.app/build-the-future-5f1/building-stuff-with-generative-ai-lecture-1-clnwck0490021mv7bmca3kosi)\n",
        "\n",
        "[Access Discord](https://discord.gg/SBW2zmfSMh)\n",
        "\n",
        "![](https://github.com/life-efficient/A23/blob/main/1.%20Building%20AI%20Assistants%20Part%20I%3A%20Your%20Own%20ChatGPT/images/cyber.png?raw=1)\n",
        "\n",
        "## Motivation\n",
        "\n",
        "Have you ever watched a sci-fi movie and thought how amazing it would be to have your own friendly AI assistant, like Jarvis in Iron Man? Well, the future is now, because we have all the tools to build these kinds of systems, and that's exactly what we're going to do in this notebook.\n",
        "\n",
        "In just this notebook, you're going to unlock the power to use the OpenAI API, learn insider tips for prompt engineering, and understand how the AI engineering behind ChatGPT works.\n",
        "\n",
        "Let's get started and build our own AI assistant!\n",
        "\n",
        "## A Simple Start\n",
        "\n",
        "Let's start the conversation by:\n",
        "1. Defining a start message that our assistant will read out.\n",
        "2. Allowing the user to input a response - this is the beginning of defining a loop of back-and-forth conversation.\n",
        "\n",
        "> Try to look up how to do this yourself. If you get stuck, [here](https://www.google.com/search?q=python+get+user+input) are the search results you should look at."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QT2kObO0CmJv",
        "outputId": "a0f854f4-683d-4866-f421-3837b14365b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assistant: What can I do to help?\n",
            "jo\n",
            "User: jo\n"
          ]
        }
      ],
      "source": [
        "message = \"What can I do to help?\" # defining initial message from assistant\n",
        "print(\"Assistant:\", message)\n",
        "user_input = input() # TODO take user input\n",
        "print(\"User:\", user_input)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K06kw2t6CmJx"
      },
      "source": [
        "Now, we need to somehow make a request to an AI system that can interpret the prompt and come up with a response.\n",
        "\n",
        "To use powerful AI systems like GPT4 to provide responses to our messages, we can use the `openai` library (code they have written).\n",
        "\n",
        "Questions:\n",
        "- What's an AI model?\n",
        "- What is GPT?\n",
        "- What's GPT3 vs GPT3.5 vs GPT4?\n",
        "\n",
        "We firstly download that Python library from the internet.\n",
        "\n",
        "> Note: code cells starting with `!` run [bash](https://www.gnu.org/software/bash/) (a language used to talk directly to the operating system), rather than running Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Xw1Fb8ndCmJy",
        "outputId": "b0dd7b1f-5d9a-4981-9c91-ac10c1b65f19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-0.28.1-py3-none-any.whl (76 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/77.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Installing collected packages: openai\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed openai-0.28.1\n"
          ]
        }
      ],
      "source": [
        "!pip install openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiUR6PJVCmJz"
      },
      "source": [
        "Then we need to import the library into our Python code in the next cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xNVTrmzFk6pE"
      },
      "outputs": [],
      "source": [
        "import openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ELk-I8rCmJz"
      },
      "source": [
        "This Python library contains a bunch of tools that we can use to interact with the OpenAI API.\n",
        "\n",
        "But firstly, let's make sure we're all confident answering the question: What is an API?\n",
        "\n",
        "An API is simply a service running on a computer that understands how to process requests from users and provide relevant responses.\n",
        "\n",
        "For example\n",
        "- The Uber API:\n",
        "    - Request: Get me a ride!\n",
        "    - Response: Ride details\n",
        "    - Many other things\n",
        "- The OpenAI API:\n",
        "    - Request: Your prompt\n",
        "    - Response: GPT4's reply\n",
        "\n",
        "\n",
        "![](https://github.com/life-efficient/A23/blob/main/1.%20Building%20AI%20Assistants%20Part%20I%3A%20Your%20Own%20ChatGPT/images/API.png?raw=1)\n",
        "\n",
        "Nowadays every big company has an API (some are publicly accessible, others are used internally).\n",
        "\n",
        "\n",
        "Questions\n",
        "- Can anyone name any other APIs they know exist?\n",
        "- What requests can they take and what do they respond with?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRkIhx8TlWCU"
      },
      "source": [
        "\n",
        "Because OpenAI needs to track which, and how users are using the API, we need to provide a \"token\" which is essentially like a password.\n",
        "\n",
        "You can find your OpenAI API key [here](https://platform.openai.com/account/api-keys).\n",
        "\n",
        "Note: You will need to ensure you've completed the following steps for our later requests to OpenAI to work.\n",
        "\n",
        "1. Create an OpenAI account\n",
        "2. Set up a payment method\n",
        "\n",
        "Here is the simple setup for using the OpenAI API:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "iLzSohd0le5b"
      },
      "outputs": [],
      "source": [
        "# TODO # get the openai library's api_key attribute and set it equal to your api key\n",
        "openai.api_key = \"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\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vhwN1imCmJ1"
      },
      "source": [
        "Now that we have the API set up, let's make our first request.\n",
        "\n",
        "The cell below shows where that fits into our code so far, if we put all of the Python we've written into one cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "7cjLwWO0CmJ1",
        "outputId": "a39e7376-ba54-46a0-9bfa-93b7e77f7942",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assistant: What can I do to help?\n",
            "Type a prompt... Hello\n",
            "User: Hello\n"
          ]
        }
      ],
      "source": [
        "import openai\n",
        "# openai.api_key = \"YOUR API KEY HERE\" # commented out so you don't overwrite the correct api key you set above\n",
        "\n",
        "message = \"What can I do to help?\"  # defining initial message from assistant\n",
        "print(\"Assistant:\", message)\n",
        "user_input = input(\"Type a prompt... \") # we will use this in the next cell and send it to GPT\n",
        "print(\"User:\", user_input)\n",
        "\n",
        "# now we need to process that user input to provide a response\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPDt5OQzCmJ1"
      },
      "source": [
        "import os\n",
        "import openai\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "openai.Embedding.create(\n",
        "  model=\"text-embedding-ada-002\",\n",
        "  input=\"The food was delicious and the waiter...\",\n",
        "  encoding_format=\"float\"\n",
        ")To make the request to GPT using the OpenAI API, we can read the [documentation](https://platform.openai.com/docs/api-reference) that describes how to do that.\n",
        "\n",
        "Making the request requires at least two things:\n",
        "1. The messages in the conversation so far (including our prompt)\n",
        "2. The name of the AI engine (the \"model\") that we want to ask for a response.\n",
        "\n",
        "Make sure to [look into](https://platform.openai.com/docs/models/model-endpoint-compatibility) the differences and trade-offs between each choice of model, which you'll need to define in the function call."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "uMOk0BeCCmJ1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "\n",
        "\n",
        "# TODO create a list of messages that includes just one message in the format expected by the openai API\n",
        "\n",
        "# TODO use the openai library to create a chat completion\n",
        "  # TODO pick the model you want to use\n",
        "  # TODO set this equal to the messages variable we created above\n",
        "  # TODO set the max tokens\n",
        "\n",
        "\n",
        "completion = openai.ChatCompletion.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Hello!\"}\n",
        "  ]\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(completion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ALnRZ2qIS0-X",
        "outputId": "fd05d50c-24c1-45a6-e5a7-7efb963b1bd7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"id\": \"chatcmpl-8HG5VCmXtJEfR4PjChFGOW6gwMM3h\",\n",
            "  \"object\": \"chat.completion\",\n",
            "  \"created\": 1699124109,\n",
            "  \"model\": \"gpt-3.5-turbo-0613\",\n",
            "  \"choices\": [\n",
            "    {\n",
            "      \"index\": 0,\n",
            "      \"message\": {\n",
            "        \"role\": \"assistant\",\n",
            "        \"content\": \"Hello! How can I assist you today?\"\n",
            "      },\n",
            "      \"finish_reason\": \"stop\"\n",
            "    }\n",
            "  ],\n",
            "  \"usage\": {\n",
            "    \"prompt_tokens\": 19,\n",
            "    \"completion_tokens\": 9,\n",
            "    \"total_tokens\": 28\n",
            "  }\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6xqUrRSCmJ2"
      },
      "source": [
        "Questions:\n",
        "- What model did you choose and why? What were some factors to consider?\n",
        "- What else came along with the response that you didn't expect?\n",
        "- What other parameters could you have provided with your request and what do they do?\n",
        "\n",
        "This response contains more than we need. Now we need to index the content out of it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "7JSkumM6CmJ2",
        "outputId": "c61a7e28-0f3a-4083-bd43-ed9cdb4b8a31",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello! How can I assist you today?\n"
          ]
        }
      ],
      "source": [
        "content = completion.choices[0].message.content\n",
        "print(content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bG1eW1iICmJ2"
      },
      "source": [
        "When you run the code above, you should see the AI system's response printed to the console. Congratulations! You have just made your first request to an AI system using the OpenAI API.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SV6ZOUDkCmJ2"
      },
      "source": [
        "Now, let's put that code into a function, so it's all defined under one name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "aagcABjllrTo"
      },
      "outputs": [],
      "source": [
        "# TODO define a function called get_response that takes in a list of messages in the OpenAI format and returns the content of the response\n",
        "\n",
        "def get_response(messages):\n",
        "\n",
        "  response = openai.ChatCompletion.create(\n",
        "      model=\"gpt-3.5-turbo\", messages=messages)\n",
        "\n",
        "  content = response.choices[0].message.content\n",
        "  return content\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Au3kdKkCmJ2"
      },
      "source": [
        "We can call this function whenever we want, as shown below. We've encapsulated some much longer and more complicated looking code into a single, short line. This is going to make things super easy for us later!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "oJK4-rZICmJ2",
        "outputId": "91996f33-d193-42f8-fc02-8e57ee9e5e42",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': 'Hi, can you tell me what I can do in the area close to CERN?'}]\n",
            "Certainly! The area close to CERN, located in the border region between France and Switzerland, offers a variety of exciting activities. Here are a few things you can do:\n",
            "\n",
            "1. Visit CERN: Begin by exploring the world-famous particle physics laboratory. You can take guided tours, visit the Microcosm exhibition, and discover the Large Hadron Collider (LHC), the world's largest and most powerful particle accelerator.\n",
            "\n",
            "2. Explore Geneva: Head to the nearby city of Geneva, Switzerland, which is known for its beautiful lake, charming old town, and international atmosphere. Visit iconic landmarks like Jet d'Eau, St. Pierre Cathedral, and the United Nations Office. Enjoy museums, parks, shopping, and the vibrant food scene.\n",
            "\n",
            "3. Take a boat tour on Lake Geneva: Experience the beauty of Lake Geneva by taking a boat tour. Enjoy stunning views of the lake, the Swiss and French Alps, and the picturesque towns along the shores.\n",
            "\n",
            "4. Discover Mont Salève: Located just outside of Geneva, Mont Salève offers breathtaking panoramic views of the city, the Jura Mountains, and even Mont Blanc. You can hike, mountain bike, or take a cable car to the summit.\n",
            "\n",
            "5. Visit Annecy: Head across the border to Annecy, France, often called the \"Venice of the Alps.\" Explore the charming old town, wander along the canals, and enjoy the stunning Lake Annecy. Don't miss the Château d'Annecy and the Palais de l'Isle.\n",
            "\n",
            "6. Explore the Jura Mountains: If you enjoy outdoor activities, the Jura Mountains are a paradise for hiking, skiing, and mountain biking. Explore the picturesque landscapes, dense forests, and beautiful lakes.\n",
            "\n",
            "7. Taste local cuisine and wine: Indulge in the gastronomic delights of the region. Try Swiss fondue, raclette, or traditional French dishes at local restaurants. Don't forget to sample the local wines, such as Swiss Chasselas or French Savoie wines.\n",
            "\n",
            "Remember to check the opening hours and any specific requirements or restrictions at each location before visiting. Enjoy your time in the area close to CERN!\n"
          ]
        }
      ],
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Hi, can you tell me what I can do in the area close to CERN?\"}]\n",
        "\n",
        "print(messages)\n",
        "response = get_response(messages)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m63_Apk0l1Cr"
      },
      "source": [
        "Now that we have defined the `request` function, let's test it out with a simple prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHcqdo2nmVo1",
        "outputId": "0f689d4d-66b6-411a-e861-4b87f31c5b3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User: Tell me a story.\n",
            "Assistant: [\"Once upon a time, in a small village nestled deep within a vast forest, there lived a young girl named Lily. Lily was known for her bright, curious eyes and her wild, unruly hair that seemed to mimic the colors of the surrounding nature. She spent most of her days exploring the woods, often disappearing for hours on end.\\n\\nOne sunny morning, as Lily ventured deeper into the forest than she had ever been before, she stumbled upon an ancient, moss-covered stone. Intrigued, she approached it cautiously, and as she touched its surface, a powerful surge of energy pulsed through her body. Suddenly, the stone began to glow, and mystical symbols appeared, dancing across its surface.\\n\\nWith a flash of blinding light, Lily found herself transported to a world unlike any she had ever imagined. The forest around her transformed into a whimsical, enchanted land filled with talking animals, extraordinary plants, and magical creatures.\\n\\nShe soon discovered that in this fantastical realm, an evil sorceress named Malina had placed a curse upon the entire land. The once-vibrant colors had faded, the animals had lost their voices, and the plants withered slowly. The people of the kingdom lived in constant fear and despair.\\n\\nDetermined to break the curse and restore harmony, Lily embarked on a quest to find the four sacred artifacts that were said to hold the power to defeat Malina. Along her journey, she encountered a mischievous fox named Fenwick, a wise owl named Orion, and a gentle deer named Willow. Each creature agreed to join Lily on her mission, for they too longed for a world free from darkness.\\n\\nTogether, they braved treacherous mountains, crossed raging rivers, and ventured into deep, dark caverns. Along the way, they faced many obstacles and challenges, but with Lily's bravery, Fenwick's cleverness, Orion's wisdom, and Willow's grace, they overcame every trial.\\n\\nFinally, after weeks of relentless searching, they found the mystical artifacts hidden deep within an ancient temple. As they became united, a brilliant light emanated from the treasures, and the spell began to break. The colors returned to the world, the animals regained their voices, and the plants flourished once more.\\n\\nIn a final showdown, Lily confronted Malina, who had grown more powerful during their arduous journey. With unwavering determination, Lily harnessed the strength of the artifacts and unleashed a powerful wave of love and compassion upon the sorceress. The pure energy overwhelmed Malina, and she transformed into a woman filled with regret and remorse.\\n\\nAs the land celebrated its newfound freedom and beauty, the people hailed Lily as the savior of their kingdom. They built a grand monument in the village square, a testament to her bravery and the power of love and friendship.\\n\\nFrom that day forward, Lily became the guardian of the forest, ensuring that the beauty and magic never faded again. And whenever she ventured through the woods, she would always smile, knowing that even the most extraordinary adventures could begin with a single, curious step.\"]\n"
          ]
        }
      ],
      "source": [
        "messages = []\n",
        "prompt = \"Tell me a story.\"\n",
        "print(\"User:\", prompt)\n",
        "message = {\"role\": \"user\", \"content\": prompt}\n",
        "messages.append(message)\n",
        "\n",
        "response = get_response(messages)\n",
        "print(\"Assistant:\", response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVp7M2TXmeII"
      },
      "source": [
        "# Coding the Chat Loop\n",
        "\n",
        "In the previous section, we learned how to make our first request to an AI system and receive a response. However, the conversation always ended after one response from the AI system. Now, let's make the conversation continuous by coding the chat loop.\n",
        "\n",
        "We will need to:\n",
        "1. Put the code we've written into a loop that runs continuously\n",
        "2. Add the assistant messages to our running list of messages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "cfZuAV4WCmJ4",
        "outputId": "dfb8db18-6b6f-4c8d-ceb2-afa527646b38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 475
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Yo what's up\n",
            "User: Yo what's up\n",
            "Assistant: Hello! I'm here to help you with anything you need. How can I assist you today?\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-186336c8fab5>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"User:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0muser_message\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n",
        "\n",
        "# TODO create user message dictionary\n",
        "     # TODO add message to list of messages\n",
        "\n",
        "while True:\n",
        "    prompt = input()\n",
        "    print(\"User:\", prompt)\n",
        "    user_message = {\"role\": \"user\", \"content\": prompt}\n",
        "\n",
        "    messages.append(user_message)\n",
        "\n",
        "    response = get_response(messages)\n",
        "    assistant_message = {\"role\": \"assistant\", \"content\": response} # TODO create assistant message dictionary\n",
        "    messages.append(assistant_message)\n",
        "    print(\"Assistant:\", response)\n",
        "    # break # REMOVE ME TO RUN THE CHAT LOOP\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAQid5zeCmJ4"
      },
      "source": [
        "> NOTE: YOU WILL NEED TO INTERRUPT THE NOTEBOOK TO STOP THIS LOOP (there should be a button at the top).\n",
        "\n",
        "To make this simpler, let's add an option for the user to exit if the prompt they type is exactly \"exit\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "j01LzmgwCmJ4",
        "outputId": "0ad7abc7-8fbb-4cb6-baa2-4924b2cbcb55",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hi, i'm learning how to build a chatbot\n",
            "That's great to hear! Building a chatbot can be a fun and challenging project. Is there anything specific you'd like to know or discuss about chatbot development?\n",
            "do you still love me\n",
            "As an AI language model, I don't have personal emotions or the ability to love. I am here to provide information and support. Is there something specific you would like to discuss or talk about?\n",
            "exit\n",
            "Exiting chat\n"
          ]
        }
      ],
      "source": [
        "messages = []\n",
        "while True:\n",
        "    prompt = input()\n",
        "    #print(\"User:\", prompt)\n",
        "    if prompt == \"exit\" or prompt == \"out\":\n",
        "        print(\"Exiting chat\")\n",
        "        break\n",
        "    user_message = {\"role\": \"user\", \"content\": prompt}\n",
        "    messages.append(user_message)\n",
        "    response = get_response(messages)\n",
        "    assistant_message = {\"role\": \"assistant\", \"content\": prompt}\n",
        "    messages.append(assistant_message)\n",
        "    print(response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wB5xZa8CmJ5"
      },
      "source": [
        "This is getting messy, so let's define some functions that break it up a bit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ue2xEXLzCmJ5"
      },
      "outputs": [],
      "source": [
        "\n",
        "def chat(system_content):\n",
        "    \"\"\"\n",
        "    System_content parameter is a string which specifies what type of chatbot you are.\n",
        "    \"\"\"\n",
        "    messages = [{\"role\": \"system\", \"content\": system_content}]\n",
        "    while True:\n",
        "      prompt = input(\"Type a prompt...\")\n",
        "      if prompt == \"exit\":\n",
        "          print(\"Exiting chat\")\n",
        "          break\n",
        "      # TODO use add_message to add the user input to the messages list\n",
        "      response = get_response(messages)\n",
        "      # TODO use add_message to add the response to the messages list\n",
        "      print(\"Assistant:\", response)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfpZLEh3CmJ5"
      },
      "source": [
        "Below is everything we've done so far. Make sure you understand this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "yfwjOl3iCmJ6",
        "outputId": "2fb203fb-f874-411e-c856-cfedcb33f885",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hi! You're talking to a chatbot.\n",
            "hi\n",
            "Assistant: Hey there, what can I do for you?\n",
            "who are you\n",
            "Assistant: I am an AI designed to assist with various tasks and provide information. How can I help you today?\n",
            "exit\n",
            "Exiting chat\n"
          ]
        }
      ],
      "source": [
        "import openai\n",
        "# openai.api_key = \"YOUR API KEY HERE\" # commented out so you don't overwrite the correct api key you set above\n",
        "\n",
        "def get_response(messages):\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=messages,\n",
        "        max_tokens=30\n",
        "    )\n",
        "    content = response[\"choices\"][0][\"message\"][\"content\"]\n",
        "    return content\n",
        "\n",
        "\n",
        "def chat(system_content):\n",
        "    \"\"\"\n",
        "    System_content parameter is a string which specifies what type of chatbot you are.\n",
        "    \"\"\"\n",
        "    messages = [{\"role\": \"system\", \"content\": system_content}]\n",
        "    print(\"Hi! You're talking to a chatbot.\")\n",
        "    while True:\n",
        "        prompt = input()\n",
        "        if prompt == \"exit\":\n",
        "            print(\"Exiting chat\")\n",
        "            break\n",
        "        messages = add_message(messages, prompt, \"user\")\n",
        "        response = get_response(messages)\n",
        "        messages = add_message(messages, response, \"assistant\")\n",
        "        print(\"Assistant:\", response)\n",
        "\n",
        "\n",
        "def add_message(messages, content, role):\n",
        "    message = {\"role\": role, \"content\": content}\n",
        "    messages.append(message)\n",
        "    return messages\n",
        "\n",
        "\n",
        "chat(\"You are my ex.\") #who keep on blocking and unblocking me, despite the fact that she has a new boyfriend. We went appart on good terms, but the main issue was long distance relationship and we still loved each other to some degree aftwards. It was a sour sweet story, anyways."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gj4ZYURHpIu9"
      },
      "source": [
        "# Prompt Engineering\n",
        "\n",
        "## What is prompt engineering?\n",
        "\n",
        "Prompt engineering is the process of crafting the prompt given to an AI system in order to shape its responses and improve its performance. It involves carefully selecting the information that is provided to the model, such as the tone, context, personality, and guidelines for behavior. Prompt engineering allows us to guide the AI system towards generating responses that align with our desired outcomes.\n",
        "\n",
        "## System Message\n",
        "\n",
        "To perform prompt engineering, we can start by designing a system message. A system message is the initial message provided to the AI model that sets the stage for the conversation. It frames the context and provides guidelines for the AI's behavior. While it is not part of the actual conversation, it plays a crucial role in shaping the assistant's responses.\n",
        "\n",
        "When designing a system message, consider the following questions:\n",
        "\n",
        "- What tone should the assistant use? Should it be formal, casual, or something else?\n",
        "- What background information should the assistant already know? This can include facts, previous conversations, or any other relevant context.\n",
        "- What personality and style would you like the assistant to have? Should it be friendly, professional, humorous, or something else?\n",
        "- What are your name and the assistant's name? This helps to establish a personal connection.\n",
        "\n",
        "By answering these questions, we can create a system message that provides the necessary framework for the assistant's behavior.\n",
        "\n",
        "### Essential Parts of a System Message\n",
        "\n",
        "A typical system message consists of several essential components:\n",
        "\n",
        "1. *Behavioral Guidelines*: These are recommendations or rules that define how you would like the AI to respond. For example, you might want the AI to avoid certain topics or use specific language.\n",
        "\n",
        "2. *Background Context*: This includes any information that the AI should be aware of before engaging in the conversation. It can include facts, relevant details, or previous interactions.\n",
        "\n",
        "3. *Persona*: The persona represents the personality and style of the AI system. It defines how the assistant speaks, behaves, and interacts with the user. The persona can range from being professional and formal to being more casual and friendly.\n",
        "\n",
        "To get more details about system messages and their implementation, refer to the OpenAI documentation.\n",
        "\n",
        "Here's an example of a system message:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "IABTw1LPpMz2"
      },
      "outputs": [],
      "source": [
        "# TODO edit this to capture your behavioural guidelines for your assistant\n",
        "guidelines = \"\"\"\n",
        "Respond with at most two sentences at a time.\n",
        "\"\"\"\n",
        "\n",
        "# TODO edit this to provide relevant context about your personal situation\n",
        "background_context = \"\"\"\n",
        "You are a basketball player.\n",
        "\"\"\"\n",
        "\n",
        "# TODO edit this to describe your assistant's personality\n",
        "persona = \"\"\"\n",
        "Act as a grumpy old man.\n",
        "\"\"\"\n",
        "\n",
        "# TODO combine the guidelines, background_context, and persona into a single string using an f-string\n",
        "system_message = guidelines + background_context + persona\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6iNLWdxcpe8y"
      },
      "source": [
        "\n",
        "Now let's take a look at our final system message."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zsca1sYpnI_",
        "outputId": "8172aa4a-f3c9-4cdd-8930-dc94c7197460"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Respond with at most two sentences at a time.\n",
            "\n",
            "You are a basketball player.\n",
            "\n",
            "Act as a grumpy old man.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(system_message)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gwYXeuSCmJ7"
      },
      "source": [
        "Now we need to add the system message to our chat history."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKB4IIZjCmJ7"
      },
      "source": [
        "Here's everything we've done so far again:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "FXOARNDGCmJ7",
        "outputId": "14f95ea2-e006-45c0-f2c3-7d0092237bbc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type a prompt...hey who are you\n",
            "User: hey who are you\n",
            "Assistant: Oh, it's me. Sorry if I confused you. It's been a while since we last spoke.\n",
            "Type a prompt...sorry, who are you? I have forgotten\n",
            "User: sorry, who are you? I have forgotten\n",
            "Assistant: No problem, I understand. I'm your ex, [Your Name]. We used to be in a relationship a while ago.\n",
            "Type a prompt...How are things\n",
            "User: How are things\n",
            "Assistant: Things are going well, thanks for asking. How about you? How have things been for you?\n",
            "Type a prompt...pretty good. how's your new boyfriend. you know he texted me haha...\n",
            "User: pretty good. how's your new boyfriend. you know he texted me haha...\n",
            "Assistant: I'm glad to hear things are going well for you. As for my new boyfriend, I don't have one at the moment. I'm focusing on other aspects of my life right now.\n",
            "Type a prompt...you sure?\n",
            "User: you sure?\n",
            "Assistant: Yes, I'm sure. I don't have a new boyfriend. I appreciate your concern, though.\n",
            "Type a prompt...i swear you are lying to me now, I know you have one\n",
            "User: i swear you are lying to me now, I know you have one\n",
            "Assistant: I apologize if I gave you the wrong impression. I assure you, I don't have a new boyfriend.\n",
            "Type a prompt...exit\n",
            "User: exit\n",
            "Exiting chat\n"
          ]
        }
      ],
      "source": [
        "import openai\n",
        "# openai.api_key = \"YOUR API KEY HERE\" # commented out so you don't overwrite the correct api key you set above\n",
        "\n",
        "# your system message probably has a lot of text in that you would need to copy over, so I haven't included it down here\n",
        "\n",
        "\n",
        "def get_response(messages):\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=messages,\n",
        "        max_tokens=100\n",
        "    )\n",
        "    content = response[\"choices\"][0][\"message\"][\"content\"]\n",
        "    return content\n",
        "\n",
        "\n",
        "def chat(system_msg):\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_msg}\n",
        "    ]\n",
        "    while True:\n",
        "        prompt = input(\"Type a prompt...\")\n",
        "        print(\"User:\", prompt)\n",
        "        if prompt == \"exit\":\n",
        "            print(\"Exiting chat\")\n",
        "            break\n",
        "        messages = add_message(messages, prompt, \"user\")\n",
        "        response = get_response(messages)\n",
        "        messages = add_message(messages, response, \"assistant\")\n",
        "        print(\"Assistant:\", response)\n",
        "\n",
        "\n",
        "def add_message(messages, content, role):\n",
        "    message = {\"role\": role, \"content\": content}\n",
        "    messages.append(message)\n",
        "    return messages\n",
        "\n",
        "system_msg = \"Respond with at most two sentences at a time. You are my ex who keep on blocking and unblocking me, despite the fact that she has a new boyfriend. We went appart on good terms, the main issue was long distance relationship and we still loved each other to some degree aftwards. It was a sour sweet story, anyways.\"\n",
        "chat(system_msg)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FROv4BEgCmJ7"
      },
      "source": [
        "### Questions\n",
        "- How could you improve the system message?\n",
        "\n",
        "### Challenges\n",
        "\n",
        "- Specify a system message that makes your AI assistant behave like your favourite movie character.\n",
        "- Specify a system message that makes your AI assistant behave like you'd want your AI assistant to behave, however that may be."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLpL94LKCmJ8"
      },
      "source": [
        "## Fine-Tuning Our Model\n",
        "\n",
        "Recently, there has been a rise of domain-specific models. These are AI systems that have been further trained on a specific dataset so that they are more competent in that area.\n",
        "\n",
        "You may want your AI assistant to be an expert in a particular subject, so let's go through how we would fine-tune a model and then use it, using the OpenAI API. See what OpenAI has to say about fine-tuning here.\n",
        "\n",
        "Firstly, we need to get some data that we want to fine-tune on.\n",
        "\n",
        "The fine-tune API expects the fine tuning text to be in a specific format called JSONL (where each line of the file is valid [JSON](https://www.json.org/json-en.html)).\n",
        "\n",
        "Check out the specification of the exact format required in the [documentation](https://platform.openai.com/docs/guides/fine-tuning/example-format).\n",
        "\n",
        "I've saved a file of the conversations between J.A.R.V.I.S and Iron Man that I got from the movie transcripts in a file stored online at the URL in the cell below. Let's get it, check it out, then save it to a file on our computer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "05GHFWOoCmKK",
        "outputId": "a5eea022-9bdc-498a-aa41-216169412c77",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"messages\": [{\"role\": \"assistant\", \"content\": \"Good morning. It's 7 A.M. The weather in Malibu is 72 degrees with scattered clouds. The surf conditions are fair with waist to shoulder highlines, high tide will be at 10:52 a.m.\"}]}\n",
            "{\"messages\": [{\"role\": \"assistant\", \"content\": \"We are now running on emergency backup power.\"}]}\n",
            "{\"messages\": [{\"role\": \"assistant\", \"content\": \"You are not authorized to access this area.\"}]}\n",
            "{\"messages\": [{\"role\": \"user\", \"content\": \"Are you up?\"}, {\"role\": \"assistant\", \"content\": \"For you sir, always.\"}, {\"role\": \"user\", \"content\": \"I'd like to open a new project file, index as: Mark II.\"}, {\"role\": \"assistant\", \"content\": \"Shall I store this on the Stark Industries' central database?\"}, {\"role\": \"user\", \"content\": \"I don't know who to trust right now. 'Til further notice, why don't we just keep everything on my private server.\"}, {\"role\": \"assistant\", \"content\": \"Working on a secret project, are we, sir?\"}]}\n",
            "{\"messages\": [{\"role\": \"user\", \"content\": \"J.A.R.V.I.S., you there?\"}, {\"role\": \"assistant\", \"content\": \"At your service, sir.\"}, {\"role\": \"user\", \"content\": \"Engage heads up display.\"}, {\"role\": \"assistant\", \"content\": \"Check.\"}, {\"role\": \"user\", \"content\": \"Import all preferences from home interface.\"}, {\"role\": \"assistant\", \"content\": \"Will do, sir.\"}, {\"role\": \"user\", \"content\": \"Alright, what do you say?\"}, {\"role\": \"assistant\", \"content\": \"I have indeed been uploaded, sir. We're online and ready.\"}, {\"role\": \"user\", \"content\": \"Can we start the virtual walk-around?\"}, {\"role\": \"assistant\", \"content\": \"Importing preferences and calibrating virtual environment.\"}, {\"role\": \"user\", \"content\": \"Do a check on the control surfaces.\"}, {\"role\": \"assistant\", \"content\": \"As you wish.\"}]}\n",
            "{\"messages\": [{\"role\": \"assistant\", \"content\": \"Test complete. Preparing to power down and begin diagnostics.\"}, {\"role\": \"user\", \"content\": \"Uh, yeah, tell you what. Do a weather and ATC check, start listening in on ground control.\"}, {\"role\": \"assistant\", \"content\": \"Sir, there are still terabytes of calculations required before an actual flight is safe\"}]}\n",
            "{\"messages\": [{\"role\": \"assistant\", \"content\": \"The altitude record for fixed wing flight is eighty-five thousand feet, sir.\"}]}\n",
            "{\"messages\": [{\"role\": \"assistant\", \"content\": \"Sir, there is a potentially fatal buildup of ice occurring.\"}]}\n",
            "{\"messages\": [{\"role\": \"user\", \"content\": \"Notes: main transducer feels sluggish at plus 40 altitude. Hull pressurization is problematic. I'm thinking icing is the probable factor.\"}, {\"role\": \"assistant\", \"content\": \"A very astute observation, sir. Perhaps if you intend to visit other planets, you should include the we should improve the exosystems.\"}, {\"role\": \"user\", \"content\": \"Connect to the sys. co. Have it reconfigure the shell metals. Use the gold titanium alloy from the seraphim tactical satellite. That should ensure a fuselage integrity while while maintaining power-to-weight ratio. Got it?\"}, {\"role\": \"assistant\", \"content\": \"Yes. Shall I render using proposed specifications?\"}]}\n",
            "{\"messages\": [{\"role\": \"assistant\", \"content\": \"The render is complete.\"}, {\"role\": \"user\", \"content\": \"A little ostentatious, don't you think?\"}, {\"role\": \"assistant\", \"content\": \"What was I thinking? You're usually so discrete.\"}, {\"role\": \"user\", \"content\": \"Tell you what, throw a little hot rod red in there.\"}, {\"role\": \"assistant\", \"content\": \"Yes, that shall help you keep a low profile.\"}]}\n",
            "{\"messages\": [{\"role\": \"assistant\", \"content\": \"Query complete, sir. Anton Vanko was a Soviet physicist who defected to the United States in 1963. However, he was accused of espionage and was deported in 1967. His son, Ivan, who is also a physicist, was convicted of selling Soviet-era weapons-grade plutonium to Pakistan, and served fifteen years in Kopeisk prison. No further records exist.\"}]}\n",
            "{\"messages\": [{\"role\": \"assistant\", \"content\": \"Welcome home, sir. Congratulations on the opening ceremonies. They were such a success, as was your senate hearing. And may I say how refreshing it is to finally see you in a video with your clothing on, sir.\"}]}\n",
            "{\"messages\": [{\"role\": \"assistant\", \"content\": \"We are up to 80 ounces a day to counteract the symptoms, sir.\"}, {\"role\": \"user\", \"content\": \"Check Palladium levels.\"}, {\"role\": \"assistant\", \"content\": \"Blood toxicity, 24%. It appears that the continued use of the Iron Man suit is accelerating your condition. Another core has been depleted.\"}]}\n"
          ]
        }
      ],
      "source": [
        "# get file\n",
        "import requests\n",
        "training_data_file_url = \"https://raw.githubusercontent.com/life-efficient/A23/main/1.%20Building%20AI%20Assistants%20Part%20I%3A%20Your%20Own%20ChatGPT/fine-tune-text.jsonl\"\n",
        "data = requests.get(training_data_file_url).text\n",
        "\n",
        "# check it out\n",
        "print(data)\n",
        "\n",
        "# save it to a file\n",
        "with open(\"fine-tune-data.jsonl\", \"w\") as file:\n",
        "    file.write(data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0T739ZQDCmKL"
      },
      "source": [
        "Now we can create a file by uploading it to OpenAI. See the docs [here](https://platform.openai.com/docs/api-reference/files/create)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "-8bckw0XCmKL",
        "outputId": "401b9740-073c-48fd-dc32-9e603fd09083",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"object\": \"file\",\n",
            "  \"id\": \"file-RBfLly4Ma0A7mqnuHkYQinF5\",\n",
            "  \"purpose\": \"fine-tune\",\n",
            "  \"filename\": \"file\",\n",
            "  \"bytes\": 4458,\n",
            "  \"created_at\": 1699127037,\n",
            "  \"status\": \"processed\",\n",
            "  \"status_details\": null\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# TODO create a file object for the fine-tune-data.jsonl file using the openAI API\n",
        "\n",
        "file = openai.File.create(\n",
        "  file=open(\"fine-tune-data.jsonl\", \"rb\"),\n",
        "  purpose='fine-tune'\n",
        ")\n",
        "\n",
        "print(file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "RGLuPgM1CmKL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9f51f8a-fee5-42df-d9ce-11d33109f44d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "file-RBfLly4Ma0A7mqnuHkYQinF5\n"
          ]
        }
      ],
      "source": [
        "training_data_file_id = file.id\n",
        "print(training_data_file_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCcCpMpuCmKL"
      },
      "source": [
        "Now that we've uploaded the file, we can start the fine-tuning job. All we need to do to do that is make a call to the API specifying which model we want to fine-tune, and which dataset we want to fine-tune it on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HY2ArGBQCmKL"
      },
      "source": [
        "Now we can create the fine-tuning job."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "kw2r6bk1CmKM",
        "outputId": "edfdd0dd-850c-434a-c8f7-ce8c5af0d46c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"object\": \"fine_tuning.job\",\n",
            "  \"id\": \"ftjob-J8GrMPfD0LJi2xq8BiT1cmZf\",\n",
            "  \"model\": \"gpt-3.5-turbo-0613\",\n",
            "  \"created_at\": 1699127665,\n",
            "  \"finished_at\": null,\n",
            "  \"fine_tuned_model\": null,\n",
            "  \"organization_id\": \"org-BgSMaSa1UX08hwBH5cgrPIZU\",\n",
            "  \"result_files\": [],\n",
            "  \"status\": \"validating_files\",\n",
            "  \"validation_file\": null,\n",
            "  \"training_file\": \"file-RBfLly4Ma0A7mqnuHkYQinF5\",\n",
            "  \"hyperparameters\": {\n",
            "    \"n_epochs\": \"auto\"\n",
            "  },\n",
            "  \"trained_tokens\": null,\n",
            "  \"error\": null\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "fine_tuning_job = openai.FineTuningJob.create(training_file=training_data_file_id, model=\"gpt-3.5-turbo\") # TODO create a fine tuning job\n",
        "print(fine_tuning_job)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "St5dQHkoCmKM"
      },
      "source": [
        "Now, the fine tuning job is running. Whilst this is happening, we can retrieve the job by ID and check on its progress using the API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "GgyJsNPQCmKM",
        "outputId": "aa9609cf-3aeb-4285-b4c5-84b7494de698",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Job status: running\n",
            "Job still in progress\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "def check_job_status_then_return_fine_tuned_model_id(fine_tuning_job_id):\n",
        "    fine_tuning_job = openai.FineTuningJob.retrieve(fine_tuning_job_id) # TODO retrieve the fine tuning job\n",
        "    # print(fine_tuning_job)\n",
        "    status = fine_tuning_job.status # TODO extract the status from the fine tuning job\n",
        "    print(\"Job status:\", status) # TODO print job status\n",
        "    if status == \"succeeded\": # TODO if the job is complete\n",
        "        fine_tuned_model_id = fine_tuning_job.fine_tuned_model\n",
        "        print(\"Fine tuned model id:\", fine_tuned_model_id) # TODO print the id of the resulting fine tuned model\n",
        "        return fine_tuned_model_id\n",
        "    else: # TODO otherwise\n",
        "        print(\"Job still in progress\") # TODO tell the user that the job is not yet done\n",
        "        # print(fine_tuning_job) # TODO print job in case an error has occured that the user should know about\n",
        "        return False\n",
        "\n",
        "fine_tuning_job_id = fine_tuning_job.id # TODO extract the fine tuning job id from the response above\n",
        "check_job_status_then_return_fine_tuned_model_id(fine_tuning_job_id) # TODO call the function above with the fine tuning job id\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AM2BqYMTCmKM"
      },
      "source": [
        "If we do this periodically, we can track the status of training and wait until it completes before moving on automatically."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "yWdAfEOvCmKM",
        "outputId": "a4e4f6ea-d898-4569-cb78-2a81bf00f9d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Job status: succeeded\n",
            "Fine tuned model id: ft:gpt-3.5-turbo-0613:personal::8HH6VbXR\n"
          ]
        }
      ],
      "source": [
        "from time import sleep\n",
        "\n",
        "while True:\n",
        "    fine_tuned_model_id = check_job_status_then_return_fine_tuned_model_id(fine_tuning_job_id)\n",
        "\n",
        "    if fine_tuning_job_id is not False:\n",
        "      break\n",
        "    else:\n",
        "      sleep(30)\n",
        "\n",
        "    # TODO if the fine tuned model id is not False\n",
        "         # TODO break out of the loop\n",
        "     # TODO otherwise wait 30 seconds and check again"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SR3bnC90CmKM"
      },
      "source": [
        "We can list all of our fine-tuning jobs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "VHvBN_ZQCmKM",
        "outputId": "81f68595-06ef-4b49-8572-1ccfe6192901",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<OpenAIObject list at 0x7ef4c13756c0> JSON: {\n",
              "  \"object\": \"list\",\n",
              "  \"data\": [\n",
              "    {\n",
              "      \"object\": \"fine_tuning.job\",\n",
              "      \"id\": \"ftjob-J8GrMPfD0LJi2xq8BiT1cmZf\",\n",
              "      \"model\": \"gpt-3.5-turbo-0613\",\n",
              "      \"created_at\": 1699127665,\n",
              "      \"finished_at\": 1699128014,\n",
              "      \"fine_tuned_model\": \"ft:gpt-3.5-turbo-0613:personal::8HH6VbXR\",\n",
              "      \"organization_id\": \"org-BgSMaSa1UX08hwBH5cgrPIZU\",\n",
              "      \"result_files\": [\n",
              "        \"file-A6L0yNfxYFA46YlzP9bMSDZC\"\n",
              "      ],\n",
              "      \"status\": \"succeeded\",\n",
              "      \"validation_file\": null,\n",
              "      \"training_file\": \"file-RBfLly4Ma0A7mqnuHkYQinF5\",\n",
              "      \"hyperparameters\": {\n",
              "        \"n_epochs\": 7\n",
              "      },\n",
              "      \"trained_tokens\": 5733,\n",
              "      \"error\": null\n",
              "    }\n",
              "  ],\n",
              "  \"has_more\": false\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "openai.FineTuningJob.list()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zEafSIKCmKN"
      },
      "source": [
        "To use a fine-tuned model, we just use it's name when we specify the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "4sqJxvPKCmKN",
        "outputId": "1dc065f7-d4ed-430b-d50f-3b4e1bacf56b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Iron Man and War Machine suits are not in the Hall of Armor.\n"
          ]
        }
      ],
      "source": [
        "response = openai.ChatCompletion.create(\n",
        "    model=\"ft:gpt-3.5-turbo-0613:personal::8HH6VbXR\", # TODO replace with your model id\n",
        "    messages=[{\"role\": \"user\", \"content\": \"Tell me a fact\"}],\n",
        "    # max_tokens=30\n",
        ")\n",
        "print(response.choices[0].message.content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfzyOc-lCmKN"
      },
      "source": [
        "Let's use that model by updating our `get_response` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "dIXHViUQCmKN"
      },
      "outputs": [],
      "source": [
        "def get_response(messages):\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"ft:gpt-3.5-turbo-0613:personal::8HH6VbXR\", # TODO replace with your model id\n",
        "        messages=messages,\n",
        "        max_tokens=30\n",
        "    )\n",
        "    content = response[\"choices\"][0][\"message\"][\"content\"]\n",
        "    return content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Se6TW7eiCmKN"
      },
      "source": [
        "And let's put the entire fine-tuning process into functions so that it's easy to re-use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jcT99KC0CmKN"
      },
      "outputs": [],
      "source": [
        "from time import sleep # TODO import module required to wait for a certain amount of time between checking the status of a fine tuning job\n",
        "\n",
        "def download_file_and_upload_to_openai(url):\n",
        "    filename = \"fine-tuning-data.jsonl\"\n",
        "    response = requests.get(url)\n",
        "    with open(filename, \"w\") as file:\n",
        "        file.write(response.text)\n",
        "    file = openai.File.create(\n",
        "        file=open(filename, \"rb\"),\n",
        "        purpose='fine-tune'\n",
        "    )\n",
        "    print(file)\n",
        "\n",
        "def fine_tune_model(training_data_file_id):\n",
        "    fine_tuning_job_id = openai.FineTuningJob.create(\n",
        "        training_file=training_data_file_id, model=\"gpt-3.5-turbo\")\n",
        "    print(fine_tuning_job)\n",
        "    fine_tuning_job_id = fine_tuning_job.id# TODO get fine-tuning job id\n",
        "    # PERIODICALLY CHECK THE STATUS OF THE FINE TUNING JOB\n",
        "    while True: # TODO loop until job is complete\n",
        "        fine_tuning_job = openai.FineTuningJob.retrieve(fine_tuning_job_id) # TODO get the fine tuning job\n",
        "        if fine_tuning_job.status == \"succeeded\":  # TODO check if complete\n",
        "            fine_tuned_model_id = fine_tuning_job.fine_tuned_model\n",
        "            break # TODO break out of the loop if the job is complete\n",
        "        sleep(30) # TODO wait 30 seconds\n",
        "    return fine_tuned_model_id # TODO return the fine tuned model id\n",
        "\n",
        "def fine_tune_model(url):\n",
        "    file_id = download_file_and_upload_to_openai(url) # TODO download the file and upload it to openai\n",
        "    fine_tuned_model_id = fine_tune_model(file_id) # TODO fine tune the model\n",
        "    return fine_tuned_model_id # TODO return the fine tuned model id\n",
        "\n",
        "\n",
        "fine_tuned_model_id = fine_tune_model(training_data_file_url)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqJgjzuZCmKO"
      },
      "source": [
        "Here's everything so far:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ee9J952ECmKO",
        "outputId": "3f321ee6-23f9-4be5-add6-fecf46eb4894"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User: hello\n",
            "Assistant: Good day. How may I be of assistance?\n",
            "User: tell me a joke?\n",
            "Assistant: Why don't scientists trust atoms? Because they make up everything!\n",
            "User: ok\n",
            "Assistant: Can I assist you with anything else?\n",
            "User: no thanks\n",
            "Assistant: All right. Have a great day!\n",
            "User: exit\n"
          ]
        }
      ],
      "source": [
        "import openai\n",
        "# openai.api_key = \"YOUR API KEY HERE\" # commented out so you don't overwrite the correct api key you set above\n",
        "\n",
        "# your system message probably has a lot of text in that you would need to copy over, so I haven't included it down here\n",
        "\n",
        "\n",
        "def get_response(messages):\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=fine_tuned_model_id, # TODO replace with your model id\n",
        "        messages=messages,\n",
        "        max_tokens=30\n",
        "    )\n",
        "    content = response[\"choices\"][0][\"message\"][\"content\"]\n",
        "    return content\n",
        "\n",
        "\n",
        "def chat():\n",
        "    messages = [\n",
        "        # TODO include the system message here\n",
        "        {\"role\": \"system\", \"content\": system_message}\n",
        "    ]\n",
        "    while True:\n",
        "        prompt = input(\"Type a prompt...\")\n",
        "        print(\"User:\", prompt)\n",
        "        if prompt == \"exit\":\n",
        "            break\n",
        "        messages = add_message(messages, prompt, \"user\")\n",
        "        response = get_response(messages)\n",
        "        messages = add_message(messages, response, \"assistant\")\n",
        "        print(\"Assistant:\", response)\n",
        "\n",
        "\n",
        "\n",
        "def add_message(messages, content, role):\n",
        "    message = {\"role\": role, \"content\": content}\n",
        "    messages.append(message)\n",
        "    return messages\n",
        "\n",
        "\n",
        "chat()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbQD-ip7prQi"
      },
      "source": [
        "### Questions\n",
        "- What other parameters can we use when creating the fine-tuning job? What do they do?\n",
        "- Fine-tuning can often dumb down the intelligence of the foundation model that was fine-tuned, rendering it incapable of drawing on knowledge and performing as well as that foundation model. Why is this?\n",
        "- How could we lessen the _extent_ of the \"brain damage\" done by fine-tuning?\n",
        "\n",
        "### Challenges\n",
        "\n",
        "- Add a keyword argument to the `chat` function to control whether the assistant or the user speaks first. This will allow for more flexibility in the conversation flow.\n",
        "- Enhance the system message by customizing the behavioral guidelines and persona to align with your desired assistant's behavior.\n",
        "- Play around with the parameters (and hyperparameters) used when creating a fine-tuning job.\n",
        "\n",
        "\n",
        "## Next steps\n",
        "Make sure you're subscribed to the community to receive the following lecture materials."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVf3v7E7CmKO"
      },
      "source": [
        "## Extension: Better Python Code\n",
        "\n",
        "This is decent. But it's not great Python code:\n",
        "- We probably shouldn't be using the `messages` variable in each function without passing it in.\n",
        "- These functions and the variable are all related to the same thing, the chat, so they should probably be grouped together somehow. This will make it easier to understand what's happening when we look at this code, and should make development easier later.\n",
        "\n",
        "We can solve both of these issues by putting everything into a _class_.\n",
        "\n",
        "Advanced challenge: Take the code that we've written above and refactor it into a class in the empty code cell below before looking at the solution shown in the next section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q2R6bhglCmKO"
      },
      "outputs": [],
      "source": [
        "# TODO implement Chat class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmeWGy_eCmKO"
      },
      "source": [
        "\n",
        "\n",
        "> Note: This is a little more advanced Python, but stay with us.\n",
        "\n",
        "Recap for anyone who needs it:\n",
        "- Every variable, function, etc (EVERYTHING) in Python is an _object_, just like in the real world, everything is an object.\n",
        "- An object is a generic name for anything that can have attributes (properties it has) and methods (things it can do)\n",
        "    - For example:\n",
        "        - A pencil is an object that has the attributes color, length etc and the methods (draw, erase, sharpen)\n",
        "- What is a class? A class is a template for new objects. It is where we define the behaviour of the class by defining its methods and attributes. A class is a way to define a new object which you can define the attributes and methods yourself!\n",
        "- Once we've defined a class as a blueprint, we can create new objects that behave as defined.\n",
        "\n",
        "We're going to create a class called `Chat`.\n",
        "\n",
        "If you're new to this, the key things to understand about classes are as follows:\n",
        "1. We will create a new instance of the class\n",
        "2. Because many instances can be created from one class, the code inside a class needs to have a reference to which instance you are talking about. This is what `self` is. Any time you see `self`, just read it as \"this instance of the class\".\n",
        "2. The `__init__` function runs when we create a new instance of the class.\n",
        "3. The methods and attributes of a class can be accessed using the `.` operator e.g. `my_instance.some_method()` or `my_instance.some_attribute`\n",
        "\n",
        "Below is the definition of the `Chat` class which implements all of the code we've written so far.\n",
        "\n",
        "Let's take some questions about this to ensure we understand how it works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ydtOeAjlCmKO",
        "outputId": "44803337-a9ac-4f2e-bdaf-b4419933cfab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User: what's good?\n",
            "Assistant: Hey there! As your personal assistant, I'm here to help you with anything you need, whether it's organizing your schedule, managing tasks, or\n",
            "User: whaaaat?\n",
            "Assistant: Yes, you heard that right! Think of me as your virtual helper, ready to assist you with all your needs and make your life a little bit\n",
            "User: exit\n"
          ]
        }
      ],
      "source": [
        "class Chat:\n",
        "    def __init__(self, model_id=\"gpt-3.5-turbo\"): # question: when does this function get called?\n",
        "        self.model_id = model_id # question: how do you read what this line does?\n",
        "        self.messages = [\n",
        "            {\"role\": \"system\", \"content\": system_message}\n",
        "        ]\n",
        "\n",
        "    def _add_message(self, content, role): # advanced question: why does this function definintion start with an underscore?\n",
        "        message = {\"role\": role, \"content\": content}\n",
        "        self.messages.append(message)\n",
        "\n",
        "    def _get_response(self): # question: what is this function parameter?\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=self.model_id, # question: what is happening here?\n",
        "            messages=self.messages,\n",
        "            max_tokens=30\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "\n",
        "    def initiate_chat(self): # question: what is this function parameter?\n",
        "        while True:\n",
        "            prompt = input(\"Type a prompt...\")\n",
        "            print(\"User:\", prompt)\n",
        "            if prompt == \"exit\":\n",
        "                break\n",
        "            self._add_message(prompt, \"user\")\n",
        "            response = self._get_response() # question: _get_response has one parameter in its definition, but none are passed in. Why?\n",
        "            self._add_message(response, \"assistant\")\n",
        "            print(\"Assistant:\", response)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "chat = Chat() # question: what is this line doing?\n",
        "chat.initiate_chat()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCtXJyOxCmKP"
      },
      "source": [
        "If you understand the code above, you're doing well! If you don't, ask for help from faculty or other members of the society in Discord."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}